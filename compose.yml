services:
  cheshire-cat-core:
    image: ghcr.io/cheshire-cat-ai/core:latest
    container_name: cheshire_cat_core
    depends_on:
      ollama:
        condition: service_healthy
      cheshire-cat-vector-memory:
        condition: service_started
    environment:
      PYTHONUNBUFFERED: "1"
      WATCHFILES_FORCE_POLLING: "true"
      CORE_HOST: ${CORE_HOST:-localhost}
      CORE_PORT: ${CORE_PORT:-1865}
      QDRANT_HOST: ${QDRANT_HOST:-cheshire_cat_vector_memory}
      QDRANT_PORT: ${QDRANT_PORT:-6333}
      CORE_USE_SECURE_PROTOCOLS: ${CORE_USE_SECURE_PROTOCOLS:-false}
      API_KEY: ${API_KEY:-}
      LOG_LEVEL: ${LOG_LEVEL:-WARNING}
      DEBUG: ${DEBUG:-false}
      SAVE_MEMORY_SNAPSHOTS: ${SAVE_MEMORY_SNAPSHOTS:-false}
    ports:
      - "0.0.0.0:${CORE_PORT:-1865}:80"
    # This add an entry to /etc/hosts file in the container mapping host.docker.internal to the host machine IP addr, allowing the container to access services running on the host, not only on Win and Mac but also Linux. 
    # See https://docs.docker.com/desktop/networking/#i-want-to-connect-from-a-container-to-a-service-on-the-host and https://docs.docker.com/reference/cli/docker/container/run/#add-host
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./cat/static:/app/cat/static
      - ./cat/plugins:/app/cat/plugins
      - ./cat/data:/app/cat/data
    restart: always

  cheshire-cat-vector-memory:
    image: qdrant/qdrant:latest
    container_name: cheshire_cat_vector_memory
    environment:
      LOG_LEVEL: ${LOG_LEVEL:-WARNING}
    expose:
      - ${QDRANT_PORT:-6333}
    volumes:
      - ./cat/long_term_memory/vector:/qdrant/storage
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--spider", "http://localhost:6333/health"]
      interval: 10s
      timeout: 15s
      retries: 10
      start_period: 20s
    restart: always

  ollama:
    build: 
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama_cat
    restart: always
    environment:
      OLLAMA_HOST: "${OLLAMA_HOST:-0.0.0.0}:${OLLAMA_PORT-11434}"
      OLLAMA_DEBUG: ${OLLAMA_DEBUG:-false}
      OLLAMA_FLASH_ATTENTION: ${OLLAMA_FLASH_ATTENTION:-false}
      OLLAMA_KEEP_ALIVE: ${OLLAMA_KEEP_ALIVE:-"5m"}
      OLLAMA_MAX_LOADED_MODELS: ${OLLAMA_MAX_LOADED_MODELS:-1}
      OLLAMA_NUM_PARALLEL: ${OLLAMA_NUM_PARALLEL:-1}
    expose:
      - ${OLLAMA_PORT:-11434}
    volumes:
      - ./ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "test", "-f", "/tmp/model_ready"]
      interval: 10s
      timeout: 5s
      retries: 50
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
            #use only cpu without nvidia gpu
  home-assistant:
    image: homeassistant/home-assistant:latest
    container_name: home-assistant
    restart: always
    volumes:
      - ./home-assistant:/config
    expose:
      - ${HOME_ASSISTANT_PORT:-8123}
    ports:
      - "0.0.0.0:${HOME_ASSISTANT_PORT:-8123}:8123"
    environment:
      - TZ=Rome/Europe
    depends_on:
      - cheshire-cat-core
      - cheshire-cat-vector-memory
      - wyoming-whisper
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8123"]
      interval: 30s
      timeout: 10s
      retries: 5
  wyoming-whisper:
    image: rhasspy/wyoming-whisper
    ports:
      - "${WHISPER_HOST_PORT:-10300}:10300"
    volumes:
      - /whisper/data:/data
    command: ["--model", "tiny-int8", "--language", "it"]
    tty: true
    stdin_open: true